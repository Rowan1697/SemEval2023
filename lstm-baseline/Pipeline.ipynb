{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_QKdPRjUee4d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26600,
     "status": "ok",
     "timestamp": 1667294887594,
     "user": {
      "displayName": "Md Zobaer Hossain",
      "userId": "17306798615063951361"
     },
     "user_tz": -60
    },
    "id": "_QKdPRjUee4d",
    "outputId": "1f10c6ac-e4cd-4241-8f04-634e5cdc8ab2"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GjuO3c0PekwF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 500,
     "status": "ok",
     "timestamp": 1667294903612,
     "user": {
      "displayName": "Md Zobaer Hossain",
      "userId": "17306798615063951361"
     },
     "user_tz": -60
    },
    "id": "GjuO3c0PekwF",
    "outputId": "6ed0e212-b54b-4485-9026-dbdf832f0310"
   },
   "outputs": [],
   "source": [
    "# %cd \"/content/drive/MyDrive/Colab Notebooks/SemEval\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd64341c",
   "metadata": {
    "executionInfo": {
     "elapsed": 7002,
     "status": "ok",
     "timestamp": 1667294853475,
     "user": {
      "displayName": "Md Zobaer Hossain",
      "userId": "17306798615063951361"
     },
     "user_tz": -60
    },
    "id": "dd64341c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import string\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a3b7a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules import ConditionalRandomField\n",
    "from allennlp.modules.conditional_random_field import allowed_transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa3d82d6",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1667294918442,
     "user": {
      "displayName": "Md Zobaer Hossain",
      "userId": "17306798615063951361"
     },
     "user_tz": -60
    },
    "id": "aa3d82d6"
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "VECTOR_PATH = './vectors/cc.bn.300.vec'\n",
    "EMB_DIMENSION = 300\n",
    "MAX_SEQ_LENGTH = 64 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f3e9c7",
   "metadata": {
    "id": "71f3e9c7"
   },
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f44e7d86",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1667294922622,
     "user": {
      "displayName": "Md Zobaer Hossain",
      "userId": "17306798615063951361"
     },
     "user_tz": -60
    },
    "id": "f44e7d86"
   },
   "outputs": [],
   "source": [
    "class EntityDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_dir, split, lang='bn'):\n",
    "        \"\"\"Initialize the attributes of the object of the class.\"\"\"\n",
    "        \n",
    "        # data directory\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "        self.lang = lang\n",
    "        # load text dataset  \n",
    "        self.sentences, self.labels = self._read_data(data_dir, split) \n",
    "        \n",
    "        # load the glove embedding\n",
    "        self.vector_path = VECTOR_PATH\n",
    "        \n",
    "        # set the embedding dimension 50/100/300\n",
    "        self.emb_dimension = EMB_DIMENSION\n",
    "        \n",
    "        \n",
    "        # set the maximum sequence length or max tweet length\n",
    "        self.max_seq_len = MAX_SEQ_LENGTH\n",
    "        \n",
    "        # create the vocabulary from the dataset\n",
    "        self.vocab = sorted(self._create_vocabulary())\n",
    "        \n",
    "        \n",
    "        # map word or tokens to index \n",
    "        self.word_to_index = {word: idx+1 for idx, word in enumerate(sorted(self.vocab))}\n",
    "        \n",
    "        # set pad token index to 0 and unk token index last of vocab\n",
    "        self.word_to_index['[PAD]'] = 0\n",
    "        self.word_to_index['[UNK]'] = len(self.vocab)+1\n",
    "        \n",
    "        # define the entitly labels to index values\n",
    "        self.tags = sorted(self._get_tags_list())\n",
    "        self.label_to_index = {tag: idx for idx, tag in enumerate(sorted(self.tags))}\n",
    "        self.label_to_index['[PAD]'] = -1\n",
    "        \n",
    "        # create the embedding vector\n",
    "        self.word_embeddings = self._create_embedding()\n",
    "        \n",
    "       \n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the size of the dataset.\"\"\"\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return a data sample for a given index, along with the lable of the corresponding tweet\"\"\"\n",
    "        \n",
    "        \n",
    "        # - get the data sample corresponding to 'index' (use the list 'self.image_path_list')\n",
    "        data_sample = self.sentences[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        # tokenize the sentence and label\n",
    "        tokens = self._tokenize_text(data_sample)\n",
    "        labels = self._tokenize_text(label)\n",
    "\n",
    "        # use the word_to_index mapping to transform the tokens into indices and save them into an IntTensor\n",
    "        x = torch.IntTensor([self.word_to_index[word] \n",
    "                             if word in self.word_to_index \n",
    "                             else self.word_to_index[\"[UNK]\"] \n",
    "                             for word in tokens])\n",
    "        \n",
    "        # transform the variable to cuda or cpu\n",
    "        x = x.to(device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # get the index-th label and store it into a FloatTensor\n",
    "        y = [self._label_map(l) for l in labels]\n",
    "        y = torch.IntTensor(torch.stack(y))\n",
    "        # transform the variable to cuda or cpu\n",
    "        y = y.to(device)\n",
    "        # stores the text indices and the label into a dictionary\n",
    "        features = {'token_ids': x, 'labels': y}\n",
    "        \n",
    "        \n",
    "        return features\n",
    "\n",
    "    \n",
    "    def _create_embedding(self):\n",
    "        \n",
    "        \"\"\"create a matrix containing word vectors\"\"\"\n",
    "\n",
    "        # load the glove embedding to a dict. token is the key and value is the vector\n",
    "        embeddings_index = {}\n",
    "#         vector_paths = os.listdir(self.vector_path)\n",
    "#         for path in vector_paths:\n",
    "#             path = os.join(self.vector_path,path)\n",
    "        with open(self.vector_path,'r') as file:\n",
    "            embeddings_index = {line.split()[0]: np.asarray(line.split()[1], dtype='float32') for line in file}\n",
    "\n",
    "        # create the embedding matrix. keep the words that only present in the dataset. \n",
    "        # each row represent one vector\n",
    "        # row index is the word map index\n",
    "        embedding_matrix = np.zeros((len(self.word_to_index) + 2, self.emb_dimension))\n",
    "        for word, i in self.word_to_index.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "        embedding_matrix[len(self.vocab)+1] = torch.randn(self.emb_dimension)\n",
    "                \n",
    "        return torch.tensor(embedding_matrix, device=device)\n",
    "        \n",
    "        \n",
    "    def _create_vocabulary(self):\n",
    "        \"\"\"Create a vocabulary of unique words from the given text files.\"\"\"\n",
    "        \n",
    "        path = 'vocab.txt'\n",
    "        with open(path, 'r') as file:\n",
    "            vocab = [line.strip() for line in file]\n",
    "\n",
    "        return list(vocab)\n",
    "    \n",
    "    def _get_tags_list(self):\n",
    "        \"\"\"Create a vocabulary of unique words from the given text files.\"\"\"\n",
    "        \n",
    "        path = 'tags.txt'\n",
    "        with open(path, 'r') as file:\n",
    "            labels = [line.strip() for line in file]\n",
    "            \n",
    "\n",
    "        return list(labels)\n",
    "\n",
    "    def _tokenize_text(self, line):\n",
    "        \"\"\"\n",
    "        Remove non-characters from the text and pads the text to max_seq_len.\n",
    "        *!* Padding is necessary for ensuring that all text_files have the same size\n",
    "        *!* This is required since DataLoader cannot handle tensors of variable length\n",
    "\n",
    "        Return a list of all tokens in the text\n",
    "        \"\"\"\n",
    "\n",
    "        tokens = line.split()\n",
    "        for i in range(self.max_seq_len - len(tokens)):\n",
    "            tokens.append('[PAD]')\n",
    "        return tokens\n",
    "    \n",
    "    def _label_map(self,label):\n",
    "        \n",
    "        \"\"\" convert to labels to one hot vectors\"\"\"\n",
    "        \n",
    "        class_num = len(self.tags)\n",
    "        \n",
    "        one_hot = torch.zeros(class_num, dtype=torch.int32)\n",
    "        idx = self.label_to_index[label]\n",
    "        if idx!=-1:\n",
    "            one_hot[idx] = 1\n",
    "        \n",
    "        return one_hot\n",
    "            \n",
    "            \n",
    "    \n",
    "    def _read_data(self, path, split):\n",
    "        \n",
    "        \"\"\" read txt file and return as list of strings\"\"\"\n",
    "        \n",
    "        path = f'{path}/{split}.csv'\n",
    "        \n",
    "        df = pd.read_csv(path)\n",
    "        df = df[df['lang']==self.lang]\n",
    "        \n",
    "        sents =  df['sent'].to_list()\n",
    "        labels = df['labels'].to_list()\n",
    "        \n",
    "        return sents, labels\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fd764f",
   "metadata": {
    "id": "b7fd764f"
   },
   "source": [
    "### Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86900685",
   "metadata": {
    "executionInfo": {
     "elapsed": 132377,
     "status": "ok",
     "timestamp": 1667295059686,
     "user": {
      "displayName": "Md Zobaer Hossain",
      "userId": "17306798615063951361"
     },
     "user_tz": -60
    },
    "id": "86900685"
   },
   "outputs": [],
   "source": [
    "dataset_train = EntityDataset('Dataset','train')\n",
    "dataset_test = EntityDataset('Dataset','dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9a7ac12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1667296995683,
     "user": {
      "displayName": "Md Zobaer Hossain",
      "userId": "17306798615063951361"
     },
     "user_tz": -60
    },
    "id": "a9a7ac12",
    "outputId": "63728cc3-6762-4ac6-a700-29f08351e4ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'B-AerospaceManufacturer',\n",
       " 1: 'B-AnatomicalStructure',\n",
       " 2: 'B-ArtWork',\n",
       " 3: 'B-Artist',\n",
       " 4: 'B-Athlete',\n",
       " 5: 'B-CarManufacturer',\n",
       " 6: 'B-Cleric',\n",
       " 7: 'B-Clothing',\n",
       " 8: 'B-Disease',\n",
       " 9: 'B-Drink',\n",
       " 10: 'B-Facility',\n",
       " 11: 'B-Food',\n",
       " 12: 'B-HumanSettlement',\n",
       " 13: 'B-MedicalProcedure',\n",
       " 14: 'B-Medication/Vaccine',\n",
       " 15: 'B-MusicalGRP',\n",
       " 16: 'B-MusicalWork',\n",
       " 17: 'B-ORG',\n",
       " 18: 'B-OtherLOC',\n",
       " 19: 'B-OtherPER',\n",
       " 20: 'B-OtherPROD',\n",
       " 21: 'B-Politician',\n",
       " 22: 'B-PrivateCorp',\n",
       " 23: 'B-PublicCorp',\n",
       " 24: 'B-Scientist',\n",
       " 25: 'B-Software',\n",
       " 26: 'B-SportsGRP',\n",
       " 27: 'B-SportsManager',\n",
       " 28: 'B-Station',\n",
       " 29: 'B-Symptom',\n",
       " 30: 'B-Vehicle',\n",
       " 31: 'B-VisualWork',\n",
       " 32: 'B-WrittenWork',\n",
       " 33: 'I-AerospaceManufacturer',\n",
       " 34: 'I-AnatomicalStructure',\n",
       " 35: 'I-ArtWork',\n",
       " 36: 'I-Artist',\n",
       " 37: 'I-Athlete',\n",
       " 38: 'I-CarManufacturer',\n",
       " 39: 'I-Cleric',\n",
       " 40: 'I-Clothing',\n",
       " 41: 'I-Disease',\n",
       " 42: 'I-Drink',\n",
       " 43: 'I-Facility',\n",
       " 44: 'I-Food',\n",
       " 45: 'I-HumanSettlement',\n",
       " 46: 'I-MedicalProcedure',\n",
       " 47: 'I-Medication/Vaccine',\n",
       " 48: 'I-MusicalGRP',\n",
       " 49: 'I-MusicalWork',\n",
       " 50: 'I-ORG',\n",
       " 51: 'I-OtherLOC',\n",
       " 52: 'I-OtherPER',\n",
       " 53: 'I-OtherPROD',\n",
       " 54: 'I-Politician',\n",
       " 55: 'I-PrivateCorp',\n",
       " 56: 'I-PublicCorp',\n",
       " 57: 'I-Scientist',\n",
       " 58: 'I-Software',\n",
       " 59: 'I-SportsGRP',\n",
       " 60: 'I-SportsManager',\n",
       " 61: 'I-Station',\n",
       " 62: 'I-Symptom',\n",
       " 63: 'I-Vehicle',\n",
       " 64: 'I-VisualWork',\n",
       " 65: 'I-WrittenWork',\n",
       " 66: 'O',\n",
       " 67: '_'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB = dataset_train.vocab\n",
    "indexMap = {v:k for k,v in dataset_train.label_to_index.items() if k!='[PAD]'}\n",
    "indexMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e71d350",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1667295103290,
     "user": {
      "displayName": "Md Zobaer Hossain",
      "userId": "17306798615063951361"
     },
     "user_tz": -60
    },
    "id": "2e71d350"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "015db0ec",
   "metadata": {
    "executionInfo": {
     "elapsed": 374,
     "status": "ok",
     "timestamp": 1667295105612,
     "user": {
      "displayName": "Md Zobaer Hossain",
      "userId": "17306798615063951361"
     },
     "user_tz": -60
    },
    "id": "015db0ec"
   },
   "outputs": [],
   "source": [
    "# First we define the validation set by splitting the training data into 2 subsets (90% training and 10% validation)\n",
    "n_train_examples = int(len(dataset_train)*0.9)\n",
    "n_valid_examples = len(dataset_train) - n_train_examples\n",
    "train_data, valid_data = random_split(dataset_train, [n_train_examples, n_valid_examples])\n",
    "\n",
    "# We also define the corresponding dataloaders\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee8510d5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1667295109326,
     "user": {
      "displayName": "Md Zobaer Hossain",
      "userId": "17306798615063951361"
     },
     "user_tz": -60
    },
    "id": "ee8510d5",
    "outputId": "1124457c-abab-4641-a793-c5c8362cdd44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 64])\n",
      "torch.Size([8, 64, 68])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print an example batch\n",
    "\n",
    "batch_example = next(iter(train_dataloader))\n",
    "tweet_batch_example = batch_example['token_ids']\n",
    "labels_batch_example = batch_example['labels']\n",
    "\n",
    "print(tweet_batch_example.shape)\n",
    "print(labels_batch_example.shape)\n",
    "\n",
    "len(batch_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524a8cc7",
   "metadata": {
    "id": "524a8cc7"
   },
   "source": [
    "### Create RNN model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7a5904a",
   "metadata": {
    "executionInfo": {
     "elapsed": 438,
     "status": "ok",
     "timestamp": 1667295113471,
     "user": {
      "displayName": "Md Zobaer Hossain",
      "userId": "17306798615063951361"
     },
     "user_tz": -60
    },
    "id": "e7a5904a"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, word_embeddings,\n",
    "                 max_sequence_length, num_layers, hidden_size, bidirectional, output_size, act_fn):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # embedding layer: converts tokens ids with respectve word vec\n",
    "        self.input_layer = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.input_layer.weight.data = word_embeddings\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=emb_dim, hidden_size = hidden_size, \n",
    "                           num_layers = num_layers, \n",
    "                           bidirectional=bidirectional, batch_first=True)\n",
    "        \n",
    "        if bidirectional:\n",
    "            self.direction = 2\n",
    "        else:\n",
    "            self.direction = 1\n",
    "            \n",
    "        self.layers = num_layers\n",
    "        \n",
    "        \n",
    "            \n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # output layer\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(self.direction*hidden_size, output_size),act_fn)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # get embedding \n",
    "        emb = self.input_layer(x)\n",
    "        \n",
    "        batch = x.shape[0]\n",
    "        # initialize a hidden state and cell state\n",
    "        h0,c0 = self.init_hidden(batch)\n",
    "        \n",
    "        # get output from lstm layers\n",
    "        l,_ = self.lstm(emb.float(),(h0,c0))\n",
    "        \n",
    "#         print(l.shape)\n",
    "        \n",
    "        # flatten the output\n",
    "        l = l.reshape(-1,l.shape[2])\n",
    "    \n",
    "        # get final class probabilities\n",
    "        out = self.output_layer(l)\n",
    "    \n",
    "        \n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "                \n",
    "        torch.manual_seed(0)\n",
    "        h0 = torch.randn(self.direction*self.layers, batch_size, self.hidden_size, device=device) \n",
    "        c0 = torch.randn(self.direction*self.layers, batch_size, self.hidden_size, device=device)\n",
    "\n",
    "        return h0,c0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca02cf2",
   "metadata": {
    "id": "dca02cf2"
   },
   "source": [
    "### Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d759c15",
   "metadata": {
    "executionInfo": {
     "elapsed": 463,
     "status": "ok",
     "timestamp": 1667297030386,
     "user": {
      "displayName": "Md Zobaer Hossain",
      "userId": "17306798615063951361"
     },
     "user_tz": -60
    },
    "id": "7d759c15"
   },
   "outputs": [],
   "source": [
    "def index_to_tag(labels):\n",
    "    \n",
    "    \"\"\"convert a batch of label indices to list of tags\"\"\"\n",
    "    \n",
    "    #define index to tag mapping\n",
    "#     indexMap = {0:'B', 1:'I', 2:'O'}\n",
    "    \n",
    "    #reshape labels to batch_size*MAX_SEQ_LENGTH\n",
    "    labels = labels.reshape((-1,MAX_SEQ_LENGTH))\n",
    "    \n",
    "    batchTags = []\n",
    "    \n",
    "    #convert label index to tags\n",
    "    for batch in labels:\n",
    "    \n",
    "        tags = [indexMap[idx.item()] for idx in batch]\n",
    "        \n",
    "        batchTags.append(tags)\n",
    "    \n",
    "    return batchTags\n",
    "\n",
    "def index_to_token(token_ids):\n",
    "    \n",
    "    \"\"\"convert a batch of token indices to list of strings\"\"\"\n",
    "    \n",
    "    batchSent = []\n",
    "    \n",
    "    for item in token_ids:\n",
    "    \n",
    "        sent = [VOCAB[idx-1] if idx < len(VOCAB) else 'UNK' for idx in item if idx!=0]\n",
    "        \n",
    "        batchSent.append(sent)\n",
    "    \n",
    "    return batchSent\n",
    "\n",
    "\n",
    "def print_predictions(tokens, pred_tags, true_tags):\n",
    "    \n",
    "    \n",
    "    batch_tokens = index_to_token(tokens)\n",
    "      \n",
    "    batch_pred_tags = index_to_tag(pred_tags)\n",
    "    \n",
    "    batch_true_tags = index_to_tag(true_tags)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    from colorama import Fore, Style, Back\n",
    "    \n",
    "    outputs = []\n",
    "    \n",
    "    preds = []\n",
    "    \n",
    "    true = []\n",
    "    \n",
    "    for tokens,true_tags,pred_tags in zip(batch_tokens,batch_pred_tags,batch_true_tags):\n",
    "        \n",
    "        true_tags = true_tags[:len(tokens)]\n",
    "        pred_tags = pred_tags[:len(tokens)]\n",
    "        \n",
    "        output = []\n",
    "    \n",
    "        for t,tl,pl in zip(tokens,true_tags,pred_tags):\n",
    "\n",
    "            assert len(tokens) == len(pred_tags) == len(true_tags)\n",
    "\n",
    "            if tl == pl:\n",
    "                o = f\"{t} {Back.GREEN}[{tl}][{pl}]{Style.RESET_ALL}\"\n",
    "\n",
    "            else:\n",
    "                o = f\"{t} {Back.GREEN}[{tl}]{Style.RESET_ALL}{Back.RED}[{pl}]{Style.RESET_ALL}\"\n",
    "\n",
    "\n",
    "            output.append(o)\n",
    "            \n",
    "        outputs.append(\" \".join(output))\n",
    "        preds.extend(pred_tags)\n",
    "        true.extend(true_tags)\n",
    "    \n",
    "    return outputs, preds, true\n",
    "\n",
    "\n",
    "\n",
    "def eval_lstm(model, eval_dataloader, return_predictions = False):\n",
    "    \n",
    "    model = copy.deepcopy(model)\n",
    "    # Set the model in 'evaluation' mode (this disables some layers (batch norm, dropout...) which are not needed when testing)\n",
    "    model.eval() \n",
    "    \n",
    "    predictions = []\n",
    "\n",
    "    # In evaluation phase, we don't need to compute gradients (for memory efficiency)\n",
    "    with torch.no_grad():\n",
    "        # initialize the total and correct number of labels to compute the accuracy\n",
    "        correct_labels = 0\n",
    "        total_labels = 0\n",
    "        \n",
    "        # Iterate over the dataset using the dataloader\n",
    "        for batch in eval_dataloader:\n",
    "\n",
    "            #get sentences and labels\n",
    "            sent = batch['token_ids']\n",
    "            labels = batch['labels']\n",
    "            \n",
    "            \n",
    "            #get number of class or tags\n",
    "            num_class = labels.shape[-1]\n",
    "    \n",
    "            #find the padded tokens\n",
    "            padx = (sent > 0).float()\n",
    "            \n",
    "            #reshape it to make it as the same shape with labels\n",
    "            padx = padx.reshape(-1)\n",
    "            \n",
    "            batch_size = sent.shape[0]\n",
    "            \n",
    "            \n",
    "            #count non-pad tokens\n",
    "            num_tokens = padx.sum().item()\n",
    "        \n",
    "            #count padded tokens\n",
    "            num_pad_tokens = padx.shape[0] - num_tokens\n",
    "            \n",
    "            #reshape it to make it as the same shape with model output\n",
    "            labels = labels.reshape(-1,num_class)\n",
    "            \n",
    "            # Get the predicted labels\n",
    "            y_predicted = model(sent)\n",
    "            \n",
    "            # To get the predicted labels, we need to get the max over all possible classes\n",
    "            # multiply with padx to ignore padded token predictions \n",
    "            label_predicted = torch.argmax(y_predicted.data, 1)*padx\n",
    "            labels = torch.argmax(labels, 1)*padx\n",
    "            \n",
    "\n",
    "            # Compute accuracy: count the total number of samples,\n",
    "            #and the correct labels (compare the true and predicted labels)\n",
    "            \n",
    "            total_labels += num_tokens #only added the non-padded tokens in count\n",
    "            \n",
    "            # subtract the padded tokens to ignore padded token predictions in final count\n",
    "            correct_labels += ((label_predicted == labels).sum().item() - num_pad_tokens)\n",
    "            \n",
    "            # get output\n",
    "            if return_predictions:\n",
    "                predictions.append(print_predictions(sent,label_predicted,labels))\n",
    "    \n",
    "    accuracy = 100 * correct_labels / total_labels\n",
    "    \n",
    "    if return_predictions:\n",
    "        return accuracy, predictions\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "plHZ8kgyp0d0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5355,
     "status": "ok",
     "timestamp": 1667297152072,
     "user": {
      "displayName": "Md Zobaer Hossain",
      "userId": "17306798615063951361"
     },
     "user_tz": -60
    },
    "id": "plHZ8kgyp0d0",
    "outputId": "dee167de-d333-49cc-f52d-deb69586482d"
   },
   "outputs": [],
   "source": [
    "# !pip install colorama\n",
    "def extract_spans(tags):\n",
    "    cur_tag = None\n",
    "    cur_start = None\n",
    "    gold_spans = {}\n",
    "\n",
    "    def _save_span(_cur_tag, _cur_start, _cur_id, _gold_spans):\n",
    "        if _cur_start is None:\n",
    "            return _gold_spans\n",
    "        _gold_spans[(_cur_start, _cur_id - 1)] = _cur_tag  # inclusive start & end, accord with conll-coref settings\n",
    "        return _gold_spans\n",
    "\n",
    "    # iterate over the tags\n",
    "    for _id, nt in enumerate(tags):\n",
    "        indicator = nt[0]\n",
    "        if indicator == 'B':\n",
    "            gold_spans = _save_span(cur_tag, cur_start, _id, gold_spans)\n",
    "            cur_start = _id\n",
    "            cur_tag = nt[2:]\n",
    "            pass\n",
    "        elif indicator == 'I':\n",
    "            # do nothing\n",
    "            pass\n",
    "        elif indicator == 'O':\n",
    "            gold_spans = _save_span(cur_tag, cur_start, _id, gold_spans)\n",
    "            cur_tag = 'O'\n",
    "            cur_start = _id\n",
    "            pass\n",
    "    _save_span(cur_tag, cur_start, _id + 1, gold_spans)\n",
    "    return gold_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5603f532",
   "metadata": {
    "executionInfo": {
     "elapsed": 450,
     "status": "ok",
     "timestamp": 1667295124875,
     "user": {
      "displayName": "Md Zobaer Hossain",
      "userId": "17306798615063951361"
     },
     "user_tz": -60
    },
    "id": "5603f532"
   },
   "outputs": [],
   "source": [
    "def loss_fn(outputs, labels, mask, id_to_tag):\n",
    "    \n",
    "#     num_class = labels.shape[-1]\n",
    "# #     labels = labels.reshape(-1,num_class) \n",
    "#     print(labels.size())\n",
    "    batch = 8\n",
    "\n",
    "    num_class = 68\n",
    "    \n",
    "    crf_layer = ConditionalRandomField(num_tags=num_class, \n",
    "                            constraints=allowed_transitions(constraint_type=\"BIO\", labels=id_to_tag))\n",
    "    \n",
    "    crf_layer.to('cuda')\n",
    "    \n",
    "    loss = -crf_layer(outputs, labels, mask) / float(batch)\n",
    "    best_path = crf_layer.viterbi_tags(outputs, mask)\n",
    "\n",
    "    pred_results, pred_tags = [], []\n",
    "    for i in range(batch_size):\n",
    "        tag_seq, _ = best_path[i]\n",
    "        pred_tags.append([id_to_tag[x] for x in tag_seq])\n",
    "#         pred_results.append(extract_spans([id_to_tag[x] for x in tag_seq if x in id_to_tag]))\n",
    "\n",
    "#     self.span_f1(pred_results, metadata)\n",
    "    output = {\"loss\": loss, \"results\": pred_tags, \"path\": best_path}\n",
    "    \n",
    "    return output\n",
    "    \n",
    "#     #define cross entropy loss \n",
    "#     criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "    \n",
    "#     #reshape labels to give a flat vector of length batch_size*seq_len\n",
    "#     num_class = labels.shape[-1]\n",
    "    \n",
    "#     # reshape label to make it similar to model output\n",
    "#     labels = labels.reshape(-1,num_class) \n",
    "\n",
    "#     #get loss\n",
    "#     loss = criterion(outputs, labels.float())\n",
    "    \n",
    "#     #get non-pad index\n",
    "#     non_pad_index=[i for i in range(labels.shape[0]) if labels[i].sum()!=0]\n",
    "    \n",
    "#     #get final loss\n",
    "#     loss = loss[non_pad_index].mean()\n",
    "    \n",
    "#     return loss\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "def training_lstm(model, train_dataloader, valid_dataloader, num_epochs, learning_rate, verbose=True):\n",
    "\n",
    "    # Make a copy of the model (avoid changing the model outside this function)\n",
    "    model_tr = copy.deepcopy(model)\n",
    "    \n",
    "    \n",
    "    # Set the model in 'training' mode (ensures all parameters' gradients are computed - it's like setting 'requires_grad=True' for all parameters)\n",
    "    model_tr.train()\n",
    "    \n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.Adam(model_tr.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Initialize lists to record the training loss over epochs\n",
    "    loss_all_epochs = []\n",
    "    val_loss_all_epochs = []\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    \n",
    "    \n",
    "    accuracy = []\n",
    "    \n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Initialize the training loss for the current epoch\n",
    "        loss_current_epoch = 0\n",
    "        val_loss_epoch = 0\n",
    "        \n",
    "        # Iterate over batches using the dataloader\n",
    "        for batch_index, batch in enumerate(train_dataloader):\n",
    "            \n",
    "            label = batch['labels']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            out = model_tr.forward(batch['token_ids'])\n",
    "            l = loss_fn(out,label)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            loss_current_epoch += (l.item())\n",
    "            \n",
    "            val_loss_epoch += loss_fn(out,label).item()\n",
    "            \n",
    "            # - use the 'backward' method to compute the gradients\n",
    "            # - apply the gradient descent algorithm\n",
    "            # Also think of updating the loss at the current epoch\n",
    "\n",
    "\n",
    "        # At the end of each epoch, record and display the loss over all batches in train and val set\n",
    "        loss_current_epoch = loss_current_epoch/len(train_dataloader)\n",
    "        val_loss_epoch = val_loss_epoch/len(train_dataloader)\n",
    "        \n",
    "        loss_all_epochs.append(loss_current_epoch)\n",
    "        val_loss_all_epochs.append(val_loss_epoch)\n",
    "        \n",
    "        # \n",
    "        acc = eval_lstm(model_tr, valid_dataloader)\n",
    "        \n",
    "        accuracy.append(acc)\n",
    "        if acc > best_accuracy:\n",
    "            best_accuracy = acc\n",
    "            torch.save(model_tr.state_dict(), 'model_opt.pt')\n",
    "            \n",
    "        \n",
    "        \n",
    "        if verbose:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss_current_epoch))\n",
    "        \n",
    "    return model_tr, loss_all_epochs ,accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eaa989",
   "metadata": {
    "id": "e7eaa989"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bcc9006",
   "metadata": {
    "executionInfo": {
     "elapsed": 4088,
     "status": "ok",
     "timestamp": 1667295140606,
     "user": {
      "displayName": "Md Zobaer Hossain",
      "userId": "17306798615063951361"
     },
     "user_tz": -60
    },
    "id": "5bcc9006"
   },
   "outputs": [],
   "source": [
    "#the vocab size that is built from train set\n",
    "vocab_size = len(dataset_train.vocab)\n",
    "# the embedding dimenstion 50/100/300\n",
    "emb_dim = EMB_DIMENSION\n",
    "# get the embedding matrix\n",
    "word_embeddings = dataset_train.word_embeddings\n",
    "# max sequence length\n",
    "max_sequence_length = MAX_SEQ_LENGTH\n",
    "\n",
    "#define lstm layers\n",
    "num_layers = 5\n",
    "#define hidden size\n",
    "hidden_size = 32\n",
    "#set if LSTM should be bidirectional \n",
    "bidirectional = False\n",
    "# output size i.e class size \n",
    "output_size = len(dataset_train.tags)\n",
    "# activation function\n",
    "act_fn = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "# create a RNN  model instance. REMARK: remove .cuda() at the end if gpu is not available\n",
    "rnn = RNN(vocab_size, emb_dim, word_embeddings, max_sequence_length, \n",
    "          num_layers,hidden_size, bidirectional, output_size, act_fn).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6309f3b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55980d50",
   "metadata": {
    "id": "55980d50"
   },
   "source": [
    "### Trial & Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1fcc4998",
   "metadata": {
    "id": "1fcc4998"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(49.5805, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " 'results': [['B-ORG',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-MusicalWork'],\n",
       "  ['B-ORG',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-MusicalWork'],\n",
       "  ['B-ORG',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-MusicalWork'],\n",
       "  ['B-ORG',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-MusicalWork'],\n",
       "  ['B-ORG',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-MusicalWork'],\n",
       "  ['B-ORG',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-MusicalWork'],\n",
       "  ['B-ORG',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-MusicalWork'],\n",
       "  ['B-ORG',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-AnatomicalStructure',\n",
       "   'B-MusicalWork']],\n",
       " 'path': [([17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 16], -37.24181365966797),\n",
       "  ([17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 16], -40.913185119628906),\n",
       "  ([17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 16], -40.86955642700195),\n",
       "  ([17, 1, 1, 1, 1, 1, 1, 16], -26.544837951660156),\n",
       "  ([17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 16], -51.56633758544922),\n",
       "  ([17, 1, 1, 1, 1, 16], -18.720470428466797),\n",
       "  ([17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 16],\n",
       "   -73.93013000488281),\n",
       "  ([17, 1, 1, 1, 1, 16], -18.955299377441406)]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "o = rnn.forward(tweet_batch_example)\n",
    "# o.shape\n",
    "# o.shape\n",
    "o = o.reshape(8,max_sequence_length,68)\n",
    "# o.size()torch.argmax\n",
    "labels = torch.argmax(labels_batch_example, dim=-1)\n",
    "# labels.\n",
    "flat_labels = labels_batch_example.reshape(-1,68)\n",
    "pad_index=[1 if flat_labels[i].sum()!=0 else 0 for i in range(flat_labels.shape[0])]\n",
    "mask = torch.FloatTensor(pad_index)\n",
    "mask = mask.to('cuda')\n",
    "\n",
    "mask = mask.reshape(8,max_sequence_length)\n",
    "# mask\n",
    "# o\n",
    "out = loss_fn(o,labels, mask, indexMap)\n",
    "out\n",
    "# labels\n",
    "# l =labels_batch_example.reshape(-1,3)\n",
    "# non_pad_index=[i for i in range(l.shape[0]) if l[i].sum()!=0 ]\n",
    "# l = l[non_pad_index]\n",
    "# l.shape\n",
    "\n",
    "\n",
    "# l = labels_batch_example.reshape(-1,3)\n",
    "# l= torch.argmax(l,1)\n",
    "# out = print_predictions(tweet_batch_example,l,l)\n",
    "# print(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c2ba3b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 30])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.ones((4,5,6))\n",
    "print(t.shape)\n",
    "t = t.reshape(4,-1)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafd3b81",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1607062,
     "status": "ok",
     "timestamp": 1667296760195,
     "user": {
      "displayName": "Md Zobaer Hossain",
      "userId": "17306798615063951361"
     },
     "user_tz": -60
    },
    "id": "dafd3b81",
    "outputId": "d6ca6fed-ac65-4b28-cddc-ca192e950f0c"
   },
   "outputs": [],
   "source": [
    "\n",
    "# number of epochs\n",
    "num_epochs = 10\n",
    "# learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# train model\n",
    "model_tr, loss_all_epochs, accuracy = training_lstm(rnn, train_dataloader, valid_dataloader, num_epochs, learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a73d3a",
   "metadata": {
    "id": "56a73d3a"
   },
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91253967",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 524,
     "status": "ok",
     "timestamp": 1667296837992,
     "user": {
      "displayName": "Md Zobaer Hossain",
      "userId": "17306798615063951361"
     },
     "user_tz": -60
    },
    "id": "91253967",
    "outputId": "7716a721-f31d-46e9-dfb1-8c6e3eb80071"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "epochs = [i for i in range(num_epochs)]\n",
    "plt.plot(epochs, loss_all_epochs, 'r', label='Loss')\n",
    "plt.xlabel('epochs'), plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d5c5f2",
   "metadata": {
    "id": "c5d5c5f2"
   },
   "source": [
    "### Eval Test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011caa46",
   "metadata": {
    "executionInfo": {
     "elapsed": 4148,
     "status": "ok",
     "timestamp": 1667297172720,
     "user": {
      "displayName": "Md Zobaer Hossain",
      "userId": "17306798615063951361"
     },
     "user_tz": -60
    },
    "id": "011caa46"
   },
   "outputs": [],
   "source": [
    "acc, preds = eval_lstm(model_tr,test_dataloader,True)\n",
    "outputs=[]\n",
    "pred_labels=[]\n",
    "true_labels = []\n",
    "for o,p,t in preds:\n",
    "    outputs.extend(o)\n",
    "    pred_labels.extend(p)\n",
    "    true_labels.extend(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f021b1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 344,
     "status": "ok",
     "timestamp": 1667297178319,
     "user": {
      "displayName": "Md Zobaer Hossain",
      "userId": "17306798615063951361"
     },
     "user_tz": -60
    },
    "id": "a4f021b1",
    "outputId": "7510180e-baa9-4f6f-8988-25770a9d21b8"
   },
   "outputs": [],
   "source": [
    "print(classification_report(pred_labels,true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33614a66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 685,
     "status": "ok",
     "timestamp": 1667297209991,
     "user": {
      "displayName": "Md Zobaer Hossain",
      "userId": "17306798615063951361"
     },
     "user_tz": -60
    },
    "id": "33614a66",
    "outputId": "0d11a809-a54a-4895-b3b4-1ceafaed5ace"
   },
   "outputs": [],
   "source": [
    "for i, out in enumerate(outputs[:3]):\n",
    "    print(out)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fe70aa",
   "metadata": {
    "id": "38fe70aa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e071a7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a946480",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1730bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bn', 'de', 'en', 'es', 'fa', 'fr', 'hi', 'it', 'pt', 'sv', 'uk',\n",
       "       'zh'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data= pd.read_csv('../Dataset/train.csv')\n",
    "data.lang.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2cf81ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['lang']=='it']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4f9aa9",
   "metadata": {},
   "source": [
    "### Information Extraction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b650137b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Davlan/xlm-roberta-base-ner-hrl\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Davlan/xlm-roberta-base-ner-hrl\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3be702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the pipeline class\n",
    "\n",
    "\n",
    "from InformationExtraction import InformationExtractionPipeline\n",
    "\n",
    "\n",
    "# example spacy extractor function\n",
    "NER = spacy.load(\"en_core_web_lg\")\n",
    "def tag_extraction_from_spacy(sen, model = NER):\n",
    "    \n",
    "    annotated = model(sen)\n",
    "    extracted_names = [word.text for word in annotated.ents \n",
    "                       if word.label_=='PERSON' or word.label_=='ORG'or word.label_=='GPE']\n",
    "    \n",
    "    \n",
    "    return extracted_names\n",
    "\n",
    "\n",
    "def tag_extraction_from_LM(sen, model = nlp):\n",
    "    \n",
    "    ner_results = model(sen)\n",
    "    extracted_names = []\n",
    "    for idx in range(len(ner_results)):\n",
    "        if ner_results[idx]['entity'][0] == 'B':\n",
    "            start = ner_results[idx]['start']\n",
    "            end = ner_results[idx]['end']\n",
    "            j = idx+1\n",
    "            while j < len(ner_results):\n",
    "                if ner_results[j]['entity'][0] == 'B':\n",
    "                    break\n",
    "                elif ner_results[j]['entity'][0] == 'I':\n",
    "                    end = ner_results[j]['end']\n",
    "                j+=1\n",
    "            idx = j\n",
    "        \n",
    "            extracted_names.append(sen[start:end].strip())\n",
    "    \n",
    "    \n",
    "    return extracted_names\n",
    "\n",
    "\n",
    "# example extractor function that uses training labels \n",
    "sent_to_tag = dict(zip(data['sent'],data['labels']))\n",
    "def tag_extraction_from_tags(sent, sent_to_tag=sent_to_tag):\n",
    "\n",
    "    tags = sent_to_tag[sent]\n",
    "    sentsWithtags = [(s,t) for s,t in zip(sent.split(),tags.split())]\n",
    "    entity_list = []\n",
    "    for i,item in enumerate(sentsWithtags):\n",
    "        if 'B-' in item[1]:\n",
    "            j = i\n",
    "            entity = []\n",
    "            while j<len(sentsWithtags):\n",
    "                if sentsWithtags[j][1] =='O':\n",
    "                    break\n",
    "                entity.append(sentsWithtags[j][0])\n",
    "                j+=1\n",
    "            i = j\n",
    "         \n",
    "            entity_list.append(\" \".join(entity))\n",
    "            \n",
    "    \n",
    "\n",
    "    return entity_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a194912",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pipline object:\n",
    "#param: extractor: an entity extractor function that returns all the entities from a sentence\n",
    "#param: max_sen: define the number of sentences to be added for each detected entity\n",
    "#param: lang: define language. needed for wikipedia api\n",
    "#param: saveJson: whether to save extracted informaton as json file. Saves time if needed to run the pipeline again\n",
    "#param: loadJson: if you have saved a json file and want to use it\n",
    "#param: jsonPath: define saved json file path\n",
    "\n",
    "infoPipeline = InformationExtractionPipeline(extractor = tag_extraction_from_LM, max_sen = 2, \n",
    "                                         lang = 'it', loadJson = True, jsonPath='wiki-info-it-train.json',\n",
    "                                         saveJson=True, saveJsonpath='wiki-info-it-train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c09795",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████▋                            | 4213/16578 [10:50<14:14, 14.46it/s]"
     ]
    }
   ],
   "source": [
    "#call pipline and provide list of sentences as argument\n",
    "\n",
    "augmented = infoPipeline(data['sent'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa793c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# infoPipeline.information_extraction(data['sent'].values.tolist()[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c550fe",
   "metadata": {},
   "source": [
    "### Info Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e146f550",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['augmented_sen'] = augmented\n",
    "temp = data[data['sent']!=data['augmented_sen']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6fb1045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info Percentage: 60.05%\n"
     ]
    }
   ],
   "source": [
    "info_percent = temp.shape[0]/data.shape[0]\n",
    "print(f\"Info Percentage: {info_percent*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e4110c",
   "metadata": {},
   "source": [
    "### Save Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3a49a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../Dataset/train-wiki-it-LM.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8c2e32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

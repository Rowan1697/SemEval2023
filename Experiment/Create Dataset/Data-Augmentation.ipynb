{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e071a7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a946480",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1730bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv('../Dataset/train.csv')\n",
    "data = data[data['lang']=='fr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a25bf8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'elle  porte  le  nom  de  la  romancière  américaine  susan  sontag  (  1933  2004  )  . '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sent.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4f9aa9",
   "metadata": {},
   "source": [
    "### Information Extraction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df5bafeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b4c181c04a4a2aa07871ede5a691ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.03G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-PER', 'score': 0.9998417, 'index': 1, 'word': '▁Na', 'start': 0, 'end': 2}, {'entity': 'I-PER', 'score': 0.88056296, 'index': 2, 'word': 'der', 'start': 2, 'end': 5}, {'entity': 'I-PER', 'score': 0.999816, 'index': 3, 'word': '▁Jo', 'start': 5, 'end': 8}, {'entity': 'I-PER', 'score': 0.9998022, 'index': 4, 'word': 'kha', 'start': 8, 'end': 11}, {'entity': 'I-PER', 'score': 0.99975294, 'index': 5, 'word': 'dar', 'start': 11, 'end': 14}, {'entity': 'B-LOC', 'score': 0.99962485, 'index': 8, 'word': '▁Syria', 'start': 24, 'end': 30}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Davlan/xlm-roberta-base-ner-hrl\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Davlan/xlm-roberta-base-ner-hrl\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b5fca29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-PER', 'score': 0.99925464, 'index': 12, 'word': '▁sus', 'start': 53, 'end': 57}, {'entity': 'I-PER', 'score': 0.90367925, 'index': 13, 'word': 'an', 'start': 57, 'end': 59}, {'entity': 'I-PER', 'score': 0.9986822, 'index': 14, 'word': '▁son', 'start': 60, 'end': 64}, {'entity': 'I-PER', 'score': 0.99791914, 'index': 15, 'word': 'tag', 'start': 64, 'end': 67}]\n"
     ]
    }
   ],
   "source": [
    "example = \"elle  porte  le  nom  de  la  romancière  américaine  susan  sontag  (  1933  2004  )  . \"\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58ad02ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'B-PER', 'score': 0.99925464, 'index': 12, 'word': '▁sus', 'start': 53, 'end': 57}\n",
      " susan  sontag\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(ner_results)):\n",
    "    if ner_results[idx]['entity'][0] == 'B':\n",
    "        start = ner_results[idx]['start']\n",
    "        end = ner_results[idx]['end']\n",
    "        j = idx+1\n",
    "        while j < len(ner_results):\n",
    "            if ner_results[j]['entity'][0] == 'B':\n",
    "                break\n",
    "            elif ner_results[j]['entity'][0] == 'I':\n",
    "                end = ner_results[j]['end']\n",
    "            j+=1\n",
    "        idx = j\n",
    "        \n",
    "        print(example[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3be702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the pipeline class\n",
    "\n",
    "\n",
    "from InformationExtraction import InformationExtractionPipeline\n",
    "\n",
    "\n",
    "# example spacy extractor function\n",
    "NER = spacy.load(\"en_core_web_lg\")\n",
    "def tag_extraction_from_spacy(sen, model = NER):\n",
    "    \n",
    "    annotated = model(sen)\n",
    "    extracted_names = [word.text for word in annotated.ents \n",
    "                       if word.label_=='PERSON' or word.label_=='ORG'or word.label_=='GPE']\n",
    "    \n",
    "    \n",
    "    return extracted_names\n",
    "\n",
    "\n",
    "def tag_extraction_from_LM(sen, model = nlp):\n",
    "    \n",
    "    annotated = model(sen)\n",
    "    extracted_names = []\n",
    "    for idx in range(len(ner_results)):\n",
    "        if ner_results[idx]['entity'][0] == 'B':\n",
    "            start = ner_results[idx]['start']\n",
    "            end = ner_results[idx]['end']\n",
    "            j = idx+1\n",
    "            while j < len(ner_results):\n",
    "                if ner_results[j]['entity'][0] == 'B':\n",
    "                    break\n",
    "                elif ner_results[j]['entity'][0] == 'I':\n",
    "                    end = ner_results[j]['end']\n",
    "                j+=1\n",
    "            idx = j\n",
    "        \n",
    "            extracted_names.append(sen[start:end].strip())\n",
    "    \n",
    "    \n",
    "    return extracted_names\n",
    "\n",
    "\n",
    "# example extractor function that uses training labels \n",
    "sent_to_tag = dict(zip(data['sent'],data['labels']))\n",
    "def tag_extraction_from_tags(sent, sent_to_tag=sent_to_tag):\n",
    "\n",
    "    tags = sent_to_tag[sent]\n",
    "    sentsWithtags = [(s,t) for s,t in zip(sent.split(),tags.split())]\n",
    "    entity_list = []\n",
    "    for i,item in enumerate(sentsWithtags):\n",
    "        if 'B-' in item[1]:\n",
    "            j = i\n",
    "            entity = []\n",
    "            while j<len(sentsWithtags):\n",
    "                if sentsWithtags[j][1] =='O':\n",
    "                    break\n",
    "                entity.append(sentsWithtags[j][0])\n",
    "                j+=1\n",
    "            i = j\n",
    "         \n",
    "            entity_list.append(\" \".join(entity))\n",
    "            \n",
    "    \n",
    "\n",
    "    return entity_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81eeed65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['susan sontag']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_extraction_from_tags(\"elle  porte  le  nom  de  la  romancière  américaine  susan  sontag  (  1933  2004  )  . \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a194912",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pipline object:\n",
    "#param: extractor: an entity extractor function that returns all the entities from a sentence\n",
    "#param: max_sen: define the number of sentences to be added for each detected entity\n",
    "#param: lang: define language. needed for wikipedia api\n",
    "#param: saveJson: whether to save extracted informaton as json file. Saves time if needed to run the pipeline again\n",
    "#param: loadJson: if you have saved a json file and want to use it\n",
    "#param: jsonPath: define saved json file path\n",
    "\n",
    "infoPipeline = InformationExtractionPipeline(extractor = tag_extraction_from_tags, max_sen = 2, \n",
    "                                         lang = 'fr', loadJson = False, jsonPath='wiki-info-fr-train.json',\n",
    "                                         saveJson=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c09795",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏                                     | 85/16547 [01:06<6:24:36,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPSConnectionPool(host='fr.wikipedia.org', port=443): Read timed out. (read timeout=10.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|█████████████                     | 6346/16547 [1:04:47<3:08:54,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPSConnectionPool(host='fr.wikipedia.org', port=443): Read timed out. (read timeout=10.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|███████████████████▉             | 10021/16547 [1:37:00<4:06:48,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPSConnectionPool(host='fr.wikipedia.org', port=443): Read timed out. (read timeout=10.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████████████████████▌         | 11787/16547 [1:52:49<1:08:46,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPSConnectionPool(host='fr.wikipedia.org', port=443): Max retries exceeded with url: /w/api.php?action=query&prop=info&titles=Chancre_Du_Ch%C3%A2taigner&inprop=protection%7Ctalkid%7Cwatched%7Cwatchers%7Cvisitingwatchers%7Cnotificationtimestamp%7Csubjectid%7Curl%7Creadable%7Cpreload%7Cdisplaytitle&format=json&redirects=1 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f2b095cf880>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|█████████████████████████▉         | 12282/16547 [1:58:27<22:15,  3.19it/s]"
     ]
    }
   ],
   "source": [
    "#call pipline and provide list of sentences as argument\n",
    "\n",
    "augmented = infoPipeline(data['sent'].values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c550fe",
   "metadata": {},
   "source": [
    "### Info Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e146f550",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['augmented_sen'] = augmented\n",
    "temp = data[data['sent']!=data['augmented_sen']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6fb1045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info Percentage: 33.91%\n"
     ]
    }
   ],
   "source": [
    "info_percent = temp.shape[0]/data.shape[0]\n",
    "print(f\"Info Percentage: {info_percent*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e4110c",
   "metadata": {},
   "source": [
    "### Save Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3a49a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('./Dataset/dev-wiki-spacy.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e071a7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-25 19:03:57.868746: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-25 19:03:58.438578: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.4/lib64\n",
      "2022-12-25 19:03:58.438619: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.4/lib64\n",
      "2022-12-25 19:03:58.438624: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a946480",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e9fae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'zh'\n",
    "Set = 'train'\n",
    "extractor = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1730bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bn', 'de', 'en', 'es', 'fa', 'fr', 'hi', 'it', 'pt', 'sv', 'uk',\n",
       "       'zh'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data= pd.read_csv(f'../Dataset/{Set}.csv')\n",
    "data.lang.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6636e5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['lang']==lang]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4f9aa9",
   "metadata": {},
   "source": [
    "### Information Extraction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e40ce873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Davlan/xlm-roberta-base-ner-hrl\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Davlan/xlm-roberta-base-ner-hrl\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6eb3f016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "# from transformers import pipeline\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"jplu/tf-xlm-r-ner-40-lang\")\n",
    "# model = AutoModelForTokenClassification.from_pretrained(\"jplu/tf-xlm-r-ner-40-lang\",from_tf=True)\n",
    "# nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3be702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the pipeline class\n",
    "\n",
    "\n",
    "from InformationExtraction import InformationExtractionPipeline\n",
    "\n",
    "\n",
    "# example spacy extractor function\n",
    "NER = spacy.load(\"en_core_web_lg\")\n",
    "def tag_extraction_from_spacy(sen, model = NER):\n",
    "    \n",
    "    annotated = model(sen)\n",
    "    extracted_names = [word.text for word in annotated.ents \n",
    "                       if word.label_=='PERSON' or word.label_=='ORG'or word.label_=='GPE']\n",
    "    \n",
    "    \n",
    "    return extracted_names\n",
    "\n",
    "\n",
    "def tag_extraction_from_LM(sen, model = nlp):\n",
    "    \n",
    "    ner_results = model(sen)\n",
    "    extracted_names = []\n",
    "    for idx in range(len(ner_results)):\n",
    "        if ner_results[idx]['entity'][0] == 'B':\n",
    "            start = ner_results[idx]['start']\n",
    "            end = ner_results[idx]['end']\n",
    "            j = idx+1\n",
    "            while j < len(ner_results):\n",
    "                if ner_results[j]['entity'][0] == 'B':\n",
    "                    break\n",
    "                elif ner_results[j]['entity'][0] == 'I':\n",
    "                    end = ner_results[j]['end']\n",
    "                j+=1\n",
    "            idx = j\n",
    "        \n",
    "            extracted_names.append(sen[start:end].strip())\n",
    "    \n",
    "    \n",
    "    return extracted_names\n",
    "\n",
    "\n",
    "# example extractor function that uses training labels \n",
    "sent_to_tag = dict(zip(data['sent'],data['labels']))\n",
    "def tag_extraction_from_tags(sent, sent_to_tag=sent_to_tag):\n",
    "\n",
    "    tags = sent_to_tag[sent]\n",
    "    sentsWithtags = [(s,t) for s,t in zip(sent.split(),tags.split())]\n",
    "    entity_list = []\n",
    "    for i,item in enumerate(sentsWithtags):\n",
    "        if 'B-' in item[1]:\n",
    "            j = i\n",
    "            entity = []\n",
    "            while j<len(sentsWithtags):\n",
    "                if sentsWithtags[j][1] =='O':\n",
    "                    break\n",
    "                entity.append(sentsWithtags[j][0])\n",
    "                j+=1\n",
    "            i = j\n",
    "         \n",
    "            entity_list.append(\" \".join(entity))\n",
    "            \n",
    "    \n",
    "\n",
    "    return entity_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34af90a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['নিউটন']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_extraction_from_LM('নিউটন একজন বিজ্ঞানী')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9724e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a194912",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pipline object:\n",
    "#param: extractor: an entity extractor function that returns all the entities from a sentence\n",
    "#param: max_sen: define the number of sentences to be added for each detected entity\n",
    "#param: lang: define language. needed for wikipedia api\n",
    "#param: saveJson: whether to save extracted informaton as json file. Saves time if needed to run the pipeline again\n",
    "#param: loadJson: if you have saved a json file and want to use it\n",
    "#param: jsonPath: define saved json file path\n",
    "\n",
    "\n",
    "infoPipeline = InformationExtractionPipeline(extractor = tag_extraction_from_tags if extractor=='all' else tag_extraction_from_LM, \n",
    "                                        max_sen = 2, lang = lang, \n",
    "                                        loadJson = False, jsonPath=f'wiki-info-{lang}-{Set}.json',\n",
    "                                        saveJson=True, saveJsonpath=f'wiki-info-{lang}-{Set}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40c09795",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████████                                                                                                                                    | 564/9758 [02:42<41:53,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPSConnectionPool(host='zh.wikipedia.org', port=443): Read timed out. (read timeout=10.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█████████████████▍                                                                                                                         | 1224/9758 [07:58<39:12,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPSConnectionPool(host='zh.wikipedia.org', port=443): Read timed out. (read timeout=10.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████████████████████▊                                                                                                                    | 1598/9758 [12:02<44:28,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPSConnectionPool(host='zh.wikipedia.org', port=443): Read timed out. (read timeout=10.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                         | 7959/9758 [45:04<10:39,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPSConnectionPool(host='zh.wikipedia.org', port=443): Read timed out. (read timeout=10.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                  | 8467/9758 [49:46<07:51,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPSConnectionPool(host='zh.wikipedia.org', port=443): Max retries exceeded with url: /w/api.php?action=query&prop=info&titles=%E7%A7%8D_%E5%AD%90&inprop=protection%7Ctalkid%7Cwatched%7Cwatchers%7Cvisitingwatchers%7Cnotificationtimestamp%7Csubjectid%7Curl%7Creadable%7Cpreload%7Cdisplaytitle&format=json&redirects=1 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fc5e4c9ee60>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9758/9758 [57:57<00:00,  2.81it/s]\n"
     ]
    }
   ],
   "source": [
    "#call pipline and provide list of sentences as argument\n",
    "\n",
    "augmented = infoPipeline(data['sent'].values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c550fe",
   "metadata": {},
   "source": [
    "### Info Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e146f550",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['augmented_sen'] = augmented\n",
    "temp = data[data['sent']!=data['augmented_sen']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6fb1045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info Percentage: 2.25%\n"
     ]
    }
   ],
   "source": [
    "info_percent = temp.shape[0]/data.shape[0]\n",
    "print(f\"Info Percentage: {info_percent*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e4110c",
   "metadata": {},
   "source": [
    "### Save Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3a49a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(f'../Dataset/{Set}-wiki-{lang}-{extractor}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1387543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

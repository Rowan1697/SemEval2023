{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e071a7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a946480",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9fae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'es'\n",
    "Set = 'dev'\n",
    "extractor = 'LM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1730bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv(f'../Dataset/{Set}.csv')\n",
    "data.lang.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6636e5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['lang']==lang]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4f9aa9",
   "metadata": {},
   "source": [
    "### Information Extraction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40ce873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "# from transformers import pipeline\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Davlan/xlm-roberta-base-ner-hrl\")\n",
    "# model = AutoModelForTokenClassification.from_pretrained(\"Davlan/xlm-roberta-base-ner-hrl\")\n",
    "# nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb3f016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jplu/tf-xlm-r-ner-40-lang\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"jplu/tf-xlm-r-ner-40-lang\",from_tf=True)\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3be702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the pipeline class\n",
    "\n",
    "\n",
    "from InformationExtraction import InformationExtractionPipeline\n",
    "\n",
    "\n",
    "# example spacy extractor function\n",
    "NER = spacy.load(\"en_core_web_lg\")\n",
    "def tag_extraction_from_spacy(sen, model = NER):\n",
    "    \n",
    "    annotated = model(sen)\n",
    "    extracted_names = [word.text for word in annotated.ents \n",
    "                       if word.label_=='PERSON' or word.label_=='ORG'or word.label_=='GPE']\n",
    "    \n",
    "    \n",
    "    return extracted_names\n",
    "\n",
    "\n",
    "def tag_extraction_from_LM(sen, model = nlp):\n",
    "    \n",
    "    ner_results = model(sen)\n",
    "    extracted_names = []\n",
    "    for idx in range(len(ner_results)):\n",
    "        if ner_results[idx]['entity'][0] == 'B':\n",
    "            start = ner_results[idx]['start']\n",
    "            end = ner_results[idx]['end']\n",
    "            j = idx+1\n",
    "            while j < len(ner_results):\n",
    "                if ner_results[j]['entity'][0] == 'B':\n",
    "                    break\n",
    "                elif ner_results[j]['entity'][0] == 'I':\n",
    "                    end = ner_results[j]['end']\n",
    "                j+=1\n",
    "            idx = j\n",
    "        \n",
    "            extracted_names.append(sen[start:end].strip())\n",
    "    \n",
    "    \n",
    "    return extracted_names\n",
    "\n",
    "\n",
    "# example extractor function that uses training labels \n",
    "sent_to_tag = dict(zip(data['sent'],data['labels']))\n",
    "def tag_extraction_from_tags(sent, sent_to_tag=sent_to_tag):\n",
    "\n",
    "    tags = sent_to_tag[sent]\n",
    "    sentsWithtags = [(s,t) for s,t in zip(sent.split(),tags.split())]\n",
    "    entity_list = []\n",
    "    for i,item in enumerate(sentsWithtags):\n",
    "        if 'B-' in item[1]:\n",
    "            j = i\n",
    "            entity = []\n",
    "            while j<len(sentsWithtags):\n",
    "                if sentsWithtags[j][1] =='O':\n",
    "                    break\n",
    "                entity.append(sentsWithtags[j][0])\n",
    "                j+=1\n",
    "            i = j\n",
    "         \n",
    "            entity_list.append(\" \".join(entity))\n",
    "            \n",
    "    \n",
    "\n",
    "    return entity_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a194912",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pipline object:\n",
    "#param: extractor: an entity extractor function that returns all the entities from a sentence\n",
    "#param: max_sen: define the number of sentences to be added for each detected entity\n",
    "#param: lang: define language. needed for wikipedia api\n",
    "#param: saveJson: whether to save extracted informaton as json file. Saves time if needed to run the pipeline again\n",
    "#param: loadJson: if you have saved a json file and want to use it\n",
    "#param: jsonPath: define saved json file path\n",
    "\n",
    "\n",
    "infoPipeline = InformationExtractionPipeline(extractor = tag_extraction_from_tags if extractor=='all' else tag_extraction_from_LM, \n",
    "                                        max_sen = 2, lang = lang, \n",
    "                                        loadJson = True, jsonPath=f'wiki-info-{lang}-{Set}.json',\n",
    "                                        saveJson=True, saveJsonpath=f'wiki-info-{lang}-{Set}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c09795",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call pipline and provide list of sentences as argument\n",
    "\n",
    "augmented = infoPipeline(data['sent'].values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c550fe",
   "metadata": {},
   "source": [
    "### Info Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e146f550",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['augmented_sen'] = augmented\n",
    "temp = data[data['sent']!=data['augmented_sen']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb1045",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_percent = temp.shape[0]/data.shape[0]\n",
    "print(f\"Info Percentage: {info_percent*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e4110c",
   "metadata": {},
   "source": [
    "### Save Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a49a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(f'../Dataset/{Set}-wiki-{lang}-{extractor}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1387543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
